{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "from environment.Config import ConfigTimeSeries\n",
    "from environment.TimeSeriesModel import TimeSeriesEnvironment\n",
    "from resources.Utils import load_object, store_object\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "from resources import Utils as utils\n",
    "from resources.Plots import plot_actions\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "# custom modules\n",
    "from agents.NeuralNetwork import build_model\n",
    "from environment import BatchLearning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Trying out different reward functions to fix the 1-step problem appearing on the windowed states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SLIDE_WINDOW_SIZE = 25  # size of the slide window for SLIDE_WINDOW state and reward functions\n",
    "LAMBDA = 0.9\n",
    "\n",
    "def SlideWindowStateFuc(timeseries, timeseries_cursor, timeseries_states=None, action=None):\n",
    "    if timeseries_cursor >= SLIDE_WINDOW_SIZE:\n",
    "        return [timeseries['value'][i + 1]\n",
    "                for i in range(timeseries_cursor - SLIDE_WINDOW_SIZE, timeseries_cursor)]\n",
    "    else:\n",
    "        return np.zeros(SLIDE_WINDOW_SIZE)\n",
    "\n",
    "\n",
    "def SlideWindowRewardFuc(timeseries, timeseries_cursor, action):\n",
    "    if timeseries_cursor >= SLIDE_WINDOW_SIZE:\n",
    "        sum_anomaly = np.sum(timeseries['anomaly']\n",
    "                             [timeseries_cursor - SLIDE_WINDOW_SIZE + 1:timeseries_cursor + 1])\n",
    "        if sum_anomaly == 0:\n",
    "            if action == 0:\n",
    "                return 5  # 0.1      # true negative\n",
    "            else:\n",
    "                return -5  # 0.5     # false positive, error alarm\n",
    "\n",
    "        if sum_anomaly > 0:\n",
    "            if action == 0:\n",
    "                return -5  # false negative, miss alarm\n",
    "            else:\n",
    "                return 5  # 10      # true positive\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = keras.Sequential()  # linear stack of layers https://keras.io/models/sequential/\n",
    "    model.add(keras.layers.Dense(SLIDE_WINDOW_SIZE + 1, input_dim=SLIDE_WINDOW_SIZE,\n",
    "                                 activation='relu'))  # [Input] -> Layer 1\n",
    "    model.add(keras.layers.Dense(SLIDE_WINDOW_SIZE * 4, activation='relu'))  # Layer 3 -> [output]\n",
    "    model.add(keras.layers.Dense(SLIDE_WINDOW_SIZE * 4, activation='relu'))  # Layer 3 -> [output]\n",
    "    model.add(keras.layers.Dense(2, activation='linear'))  # Layer 3 -> [output]\n",
    "    model.compile(loss='mse',  # Loss function: Mean Squared Error\n",
    "                  optimizer=keras.optimizers.Adam(\n",
    "                      lr=0.01))  # Optimaizer: Adam (Feel free to check other options)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We need to modify our agent to use a multi step target update, so our traces\n",
    "are able to differentiate between sudden changes in the value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "\n",
    "class DDQNWAgent:\n",
    "    def __init__(self, actions, alpha, gamma, epsilon, epsilon_min, epsilon_decay):\n",
    "        self.nA = actions\n",
    "        self.memory = MemoryBuffer(max=50000)\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        # Explore/Exploit\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        # fitting param\n",
    "        self.epoch_count = 10\n",
    "        self.model = build_model()\n",
    "        self.model_target = build_model()  # Second (target) neural network\n",
    "        self.update_target_from_model()  # Update weights\n",
    "        self.hist = None\n",
    "        self.loss = []\n",
    "\n",
    "    def action(self, state):\n",
    "        # returns a random action or greedy plus a boolean if the action was choosen greedy or random\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.nA)  # Explore\n",
    "        if self.epsilon == 0:\n",
    "            action_vals = self.model_target.predict(np.array(state).reshape(1,\n",
    "                                                                            SLIDE_WINDOW_SIZE))\n",
    "            print(\"Pred 0\\n{}\".format(action_vals[0][0]))\n",
    "            print(\"Pred 1\\n{}\".format(action_vals[0][1]))\n",
    "        else:\n",
    "            action_vals = self.model.predict(np.array(state).reshape(1,\n",
    "                                                                     SLIDE_WINDOW_SIZE))  # Exploit: Use the NN to predict the correct action from this state\n",
    "        return np.argmax(action_vals)\n",
    "\n",
    "    def experience_replay(self, batch_size, lstm, step_size=25):\n",
    "        # get a sample from the memory buffer\n",
    "        minibatch = self.memory.get_exp(batch_size)\n",
    "\n",
    "        # create default input arrays for the fitting of the model\n",
    "        x = []\n",
    "        y = []\n",
    "\n",
    "        st_predict, nst_predict, nst_predict_target = self.predict_on_batch(minibatch)\n",
    "\n",
    "        index = 0\n",
    "        step = 1\n",
    "        reward_ = 0\n",
    "        for state, action, reward, nstate, done in minibatch:\n",
    "            if step <= step_size:\n",
    "#                 print(\"-------Trace Step {}--------\".format(step))\n",
    "#                 print(\"S1: {}\".format(state))\n",
    "#                 print(\"A: {}\".format(action))\n",
    "#                 print(\"R: {}\".format(reward))\n",
    "#                 print(\"S2: {}\".format(nstate))\n",
    "#                 print(\"D: {}\".format(done))\n",
    "#                 print(\"Lambda: {}\".format(LAMBDA**step))\n",
    "                reward_ = reward_ + reward * (LAMBDA**step)\n",
    "#                 print(\"R_: {}\".format(reward_))\n",
    "                x.append(state)\n",
    "                # Predict from state\n",
    "                nst_action_predict_target = nst_predict_target[index]\n",
    "                nst_action_predict_model = nst_predict[index]\n",
    "                if done == True:  # Terminal: Just assign reward\n",
    "                    target = reward_\n",
    "                else:  # Non terminal\n",
    "                    target = reward_ + self.gamma * nst_action_predict_target[\n",
    "                        np.argmax(nst_action_predict_model)]  # Using Q to get T is Double DQN\n",
    "                target_f = st_predict[index]\n",
    "                target_f[action] = target\n",
    "                y.append(target_f)\n",
    "                index += 1\n",
    "                step += 1\n",
    "            else:\n",
    "#                 print(\"Resetting Trace\")\n",
    "                step = 1\n",
    "                reward_ = 0\n",
    "        # Reshape for Keras Fit\n",
    "        x_reshape = np.array(x)\n",
    "        y_reshape = np.array(y)\n",
    "        self.hist = self.model.fit(x_reshape, y_reshape, epochs=self.epoch_count, verbose=0)\n",
    "\n",
    "    def predict_on_batch(self, batch):\n",
    "        # Convert to numpy for speed by vectorization\n",
    "        st = np.array(list(list(zip(*batch))[0]))\n",
    "        nst = np.array(list(list(zip(*batch))[3]))\n",
    "\n",
    "        # predict on the batches with the model as well as the target values\n",
    "        st_predict = self.model.predict(st)\n",
    "        nst_predict = self.model.predict(nst)\n",
    "        nst_predict_target = self.model_target.predict(nst)\n",
    "\n",
    "        return st_predict, nst_predict, nst_predict_target\n",
    "\n",
    "    def update_target_from_model(self):\n",
    "        # Update the target model from the base model\n",
    "        self.model_target.set_weights(self.model.get_weights())\n",
    "\n",
    "    def anneal_eps(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon_decay * self.epsilon)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to modify the Memory Replay, so we can store\n",
    "every transition inside our environment at least once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MemoryBuffer:\n",
    "    def __init__(self, max):\n",
    "        self.memory = deque([], maxlen=max)\n",
    "        self.memory_copy = deque([], maxlen=max)\n",
    "\n",
    "    def store(self, state, action, reward, nstate, done):\n",
    "        # Store the experience in memory\n",
    "        self.memory.append((state, action, reward, nstate, done))\n",
    "\n",
    "    def init_memory(self, env, step_size=25):\n",
    "        # time measurement for memory initialization\n",
    "        init_time = time.time()\n",
    "        # resetting environment once\n",
    "        env.reset()\n",
    "        # try to load memory from local file\n",
    "        if not os.path.isfile(\"memory.obj\"):\n",
    "            print(\"Test\")\n",
    "            #self.memory = load_object(\"memory.obj\")\n",
    "        # try to init memory by taking random steps in our environment until the deque is full\n",
    "        else:\n",
    "            # init trajectory step\n",
    "            step = 1\n",
    "            reward_ = 0\n",
    "            action_type = 0\n",
    "            while True:\n",
    "                if action_type % 2 == 0:\n",
    "                    if step <= step_size:\n",
    "                        # break if memory is full\n",
    "                        if len(self.memory) >= self.memory.maxlen:\n",
    "                            break\n",
    "                        # check if we need to reset env and still fill our memory\n",
    "                        if env.is_done(env.timeseries_cursor):\n",
    "                            env.reset()\n",
    "                            action_type = 1\n",
    "#                             print(\"---------NEW STRATEGY INIT-------\\n\\n\\n\")\n",
    "                        # get random action\n",
    "                        action = random.randrange(env.action_space_n)\n",
    "                        # take step in env and append\n",
    "                        state, action, reward, nstate, done = env.step_window(action)\n",
    "                        reward_ = reward_ + reward * (LAMBDA**step)\n",
    "                        step += 1\n",
    "                        # store our memory in class\n",
    "                        self.store(state, action, reward_, nstate, done)\n",
    "#                         print(\"TIMESTAMP: \\t{}\".format(env.timeseries_cursor))\n",
    "#                         print(state[-1], action, reward_, nstate[-1], done)\n",
    "                    else:\n",
    "#                         print(\"Starting new trajectory normal\")\n",
    "                        step = 1\n",
    "                        reward_ = 0\n",
    "                if action_type % 2 != 0:\n",
    "                    if step <= step_size:\n",
    "                        # break if memory is full\n",
    "                        if len(self.memory) >= self.memory.maxlen:\n",
    "                            break\n",
    "                        # check if we need to reset env and still fill our memory\n",
    "                        if env.is_done(env.timeseries_cursor):\n",
    "                            env.reset()\n",
    "                            action_type = 0\n",
    "#                             print(\"---------NEW STRATEGY INIT-------\\n\\n\\n\")\n",
    "                        # get random action\n",
    "                        action = random.randrange(env.action_space_n)\n",
    "                        # take step in env and append\n",
    "                        state, action, reward, nstate, done = env.step_window(action)\n",
    "                        reward_ = reward_ + reward * (LAMBDA**step)\n",
    "                        step += 1\n",
    "                        # store our memory in class\n",
    "                        self.store(state, action, reward_, nstate, done)\n",
    "#                         print(\"TIMESTAMP: \\t{}\".format(env.timeseries_cursor))\n",
    "#                         print(state[-1], action, reward_, nstate[-1], done)\n",
    "                    else:\n",
    "#                         print(\"Starting new trajectory anomaly\")\n",
    "                        step = 1\n",
    "                        reward_ = 0\n",
    "            # store our memory locally to reduce loading time on next run\n",
    "            store_object(self.memory, \"memory.obj\")\n",
    "            print(\"Memory is full, {} Samples stored. It took {} seconds\".format(len(self.memory),\n",
    "                                                                                 time.time() - init_time))\n",
    "\n",
    "    def get_exp(self, batch_size):\n",
    "        # Popping from the Memory Queue which should be filled randomly beforehand\n",
    "        experience = [self.memory.popleft() for _i in range(batch_size)]\n",
    "        for exp in experience:\n",
    "            self.memory_copy.appendleft(exp)\n",
    "        return experience\n",
    "        # return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def get_all_states(self):\n",
    "        return np.asarray([self.memory.popleft() for _i in range(self.memory.maxlen)])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Memory\n",
    "# config = ConfigTimeSeries(seperator=\",\", window=BatchLearning.SLIDE_WINDOW_SIZE)\n",
    "# env = TimeSeriesEnvironment(verbose=True, filename=\"./Test/SmallData_1.csv\", config=config, window=True)\n",
    "# env.statefunction = SlideWindowStateFuc\n",
    "# env.rewardfunction = SlideWindowRewardFuc\n",
    "# env.timeseries_cursor_init = SLIDE_WINDOW_SIZE\n",
    "# mem = MemoryBuffer(env.timeseries_labeled.shape[0] * 2)\n",
    "# mem.init_memory(env)\n",
    "# print(len(mem.memory))\n",
    "# print(mem.get_exp(256))\n",
    "# print(len(mem.memory))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The next cell is to test if we have all available actions in our environment taken at least once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulator:\n",
    "    \"\"\"\n",
    "    This class is used to train and to test the agent in its environment\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_episodes, agent, environment, update_steps):\n",
    "        self.max_episodes = max_episodes\n",
    "        self.episode = 1\n",
    "        self.agent = agent\n",
    "        self.env = environment\n",
    "        self.update_steps = update_steps\n",
    "\n",
    "        # information variables\n",
    "        self.training_scores = []\n",
    "        self.test_rewards = []\n",
    "        self.test_actions = []\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        This method is for scheduling training before testing\n",
    "        :return: True if finished\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            start = utils.start_timer()\n",
    "            start_testing = self.can_test()\n",
    "            if not start_testing:\n",
    "                info = self.training_iteration()\n",
    "                print(\"Training episode {} took {} seconds {}\".format(self.episode, utils.get_duration(start), info))\n",
    "                self.next()\n",
    "            if start_testing:\n",
    "                self.testing_iteration()\n",
    "                print(\"Testing episode {} took {} seconds\".format(self.episode, utils.get_duration(start)))\n",
    "                break\n",
    "            self.agent.anneal_eps()\n",
    "        plot_actions(self.test_actions[0], self.env.timeseries_labeled)\n",
    "        return True\n",
    "\n",
    "    def can_test(self):\n",
    "        if self.episode >= self.max_episodes:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def next(self):\n",
    "        self.episode += 1\n",
    "\n",
    "    def training_iteration(self):\n",
    "        rewards = 0\n",
    "        state = self.env.reset()\n",
    "        for idx in range(len(\n",
    "                self.env.timeseries_labeled)):\n",
    "            action = self.agent.action(state)\n",
    "            state, action, reward, nstate, done = self.env.step_window(action)\n",
    "            rewards += reward\n",
    "            self.agent.memory.store(state, action, reward, nstate, done)\n",
    "            state = nstate\n",
    "            if done:\n",
    "                self.training_scores.append(rewards)\n",
    "                break\n",
    "            # Experience Replay\n",
    "            if len(self.agent.memory) > self.agent.batch_size:\n",
    "                self.agent.experience_replay(self.agent.batch_size, lstm=False)\n",
    "        # self.agent.memory.memory = self.agent.memory.memory_copy\n",
    "        # Target Model Update\n",
    "        if self.episode % self.update_steps == 0:\n",
    "            self.agent.update_target_from_model()\n",
    "            return \"Update Target Model\"\n",
    "        return \"\"\n",
    "\n",
    "    def testing_iteration(self):\n",
    "        self.env.timeseries_cursor_init = 0\n",
    "        rewards = 0\n",
    "        actions = []\n",
    "        state = self.env.reset()\n",
    "        self.agent.epsilon = 0\n",
    "        for idx in range(len(\n",
    "                self.env.timeseries_labeled)):\n",
    "            action = self.agent.action(state)\n",
    "            actions.append(action)\n",
    "            state, action, reward, nstate, done = self.env.step_window(action)\n",
    "            if idx >= 800 and idx <= 1000:\n",
    "                print(\"At Timestamp: \" + str(idx))\n",
    "                print(\"State:\\t\\t\\t\" + str(state))\n",
    "                print(\"Action:\\t\\t\\t\" + str(action))\n",
    "                print(\"Reward:\\t\\t\\t\" + str(reward))\n",
    "\n",
    "            rewards += reward\n",
    "            state = nstate\n",
    "            if done:\n",
    "                actions.append(action)\n",
    "                self.test_rewards.append(rewards)\n",
    "                self.test_actions.append(actions)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeSeries from: ./Test/SmallData_1.csv\n",
      " Header(labeled):\n",
      "    value  anomaly\n",
      "0      0        0\n",
      "1      0        0\n",
      "2      4        0 \n",
      "Header(unlabeled):\n",
      "    value\n",
      "0      0\n",
      "1      0\n",
      "2      4 \n",
      "Rows:\n",
      " 1424\n",
      "MeanValue:\n",
      " 40.39\n",
      "MaxValue:\n",
      " 452\n",
      "MinValue:\n",
      " 0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "defaultState() missing 1 required positional argument: 'action'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-34-4d1887ac8d83>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[0mdqn\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mDDQNWAgent\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0menv\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0maction_space_n\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0.001\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0.9\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0.9\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 11\u001B[1;33m \u001B[0mdqn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmemory\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minit_memory\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0menv\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     12\u001B[0m \u001B[0msimulation\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mSimulator\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m11\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdqn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0menv\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m5\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[0msimulation\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-31-61afa5c82aaa>\u001B[0m in \u001B[0;36minit_memory\u001B[1;34m(self, env, step_size)\u001B[0m\n\u001B[0;32m     12\u001B[0m         \u001B[0minit_time\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     13\u001B[0m         \u001B[1;31m# resetting environment once\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 14\u001B[1;33m         \u001B[0menv\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     15\u001B[0m         \u001B[1;31m# try to load memory from local file\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     16\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0misfile\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"memory.obj\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mN:\\MasterPredictions\\environment\\TimeSeriesModel.py\u001B[0m in \u001B[0;36mreset\u001B[1;34m(self, cursor_init)\u001B[0m\n\u001B[0;32m     95\u001B[0m             \u001B[0minit_state\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstatefunction\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtimeseries_cursor\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     96\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 97\u001B[1;33m             \u001B[0minit_state\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstatefunction\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtimeseries_labeled\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtimeseries_cursor\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     98\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0minit_state\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     99\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: defaultState() missing 1 required positional argument: 'action'"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# Create the agent\n",
    "config = ConfigTimeSeries(seperator=\",\", window=BatchLearning.SLIDE_WINDOW_SIZE)\n",
    "env = TimeSeriesEnvironment(verbose=True, filename=\"./Test/SmallData_1.csv\", config=config, window=True)\n",
    "env.statefunction = SlideWindowStateFuc\n",
    "env.rewardfunction = SlideWindowRewardFuc\n",
    "#env.timeseries_cursor_init = SLIDE_WINDOW_SIZE\n",
    "\n",
    "dqn = DDQNWAgent(env.action_space_n, 0.001, 0.9, 1, 0, 0.9)\n",
    "dqn.memory.init_memory(env)\n",
    "simulation = Simulator(11, dqn, env, 5)\n",
    "simulation.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (MasterPredictions)",
   "language": "python",
   "name": "pycharm-689bdd82"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}